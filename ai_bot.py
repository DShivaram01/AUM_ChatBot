# -*- coding: utf-8 -*-
"""AI_Bot.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/15HbvYpaEGU8LYRdlZ9cziRZfgwbBB5tI

##Use GPU by switching the runtime to T4. Running the bot on CPU causes slow replies.

###execute the modules only once
"""

pip install sentence-transformers faiss-cpu

from google.colab import files
import shutil

from google.colab import drive
drive.mount('/content/drive')

"""Hello Professor,

This notebook contains an interactive chatbot designed to help students understand core concepts in Data Structures and Algorithms, as well as provide information about faculty members and their academic work.

To use the assistant:

1. When prompted, please choose a **category** (e.g., DSA or Faculty).
2. Then select a **topic** under that category.
3. You can ask any relevant question related to the selected topic.
4. If you'd like to change the topic or category, just type:

   - `change topic` ‚Üí to switch to another topic
   - `menu` or `change category` ‚Üí to go back to the category selection
   - `exit` ‚Üí to end the session

The chatbot retrieves relevant information and generates helpful answers using a retrieval-augmented approach.

All your questions and the chatbot‚Äôs responses will be logged in:
üìÅ `MyDrive/2_AI_Tutor/chatbot_logs.csv`

Thank you, and please feel free to explore and ask any questions.

"""

##Run only once!!
##Upload the knowledge base .json file when prompted.
##Provide access to google drive when prompted.
##Loading from Google Drive is time efficient compared to loading from local system

from google.colab import files
import shutil
import os
import json
import faiss
import shutil
import numpy as np
from collections import defaultdict
from sentence_transformers import SentenceTransformer

# === Mount Google Drive ===
from google.colab import drive, files
drive.mount('/content/drive')

# === Define working directory in Drive
drive_folder = "/content/drive/MyDrive/AI_Tutor_Big_Data"
os.makedirs(drive_folder, exist_ok=True)

# === Define updated filename
input_file_path = os.path.join(drive_folder, "unified_knowledge_base_updated_names (2).json")

# === Upload only if file not already present
if not os.path.exists(input_file_path):
    print("üìÅ Please upload `unified_knowledge_base_updated_names.json` from your computer")
    uploaded = files.upload()
    uploaded_filename = list(uploaded.keys())[0]
    shutil.move(uploaded_filename, input_file_path)
    print(f"‚úÖ File moved to Drive: {input_file_path}")
else:
    print(f"üìÑ Using existing file in Drive: {input_file_path}")

# === Load the input file from Drive
with open(input_file_path, "r") as f:
    chunks = json.load(f)

# === Load embedding model
model = SentenceTransformer("all-MiniLM-L6-v2")

# === Prepare texts and metadata
texts = [chunk["content"] for chunk in chunks]
metadata = [{
    "id": chunk.get("id"),
    "name": chunk.get("name"),
    "category": chunk.get("category"),
    "topic": chunk.get("topic"),
    "subtopic": chunk.get("subtopic"),
    "department": chunk.get("department"),
    "research_areas": chunk.get("research_areas"),
    "web_link": chunk.get("web_link"),
    "content": chunk.get("content")
} for chunk in chunks]

# === Generate embeddings and build FAISS index
embeddings = model.encode(texts, convert_to_numpy=True)
index = faiss.IndexFlatL2(embeddings.shape[1])
index.add(embeddings)

# === Save outputs to Drive
faiss.write_index(index, os.path.join(drive_folder, "unified_faiss_index.bin"))
with open(os.path.join(drive_folder, "unified_faiss_metadata.json"), "w") as f:
    json.dump(metadata, f, indent=2)

# === Create topic-subtopic structure
topic_map = defaultdict(set)
for m in metadata:
    topic_map[m["topic"]].add(m["subtopic"])
topic_structure = {k: sorted(v) for k, v in topic_map.items()}

with open(os.path.join(drive_folder, "unified_topic_subtopic_structure.json"), "w") as f:
    json.dump(topic_structure, f, indent=2)

print("‚úÖ All outputs saved to Google Drive ‚Üí 2_AI_Tutor/")

"""## Its best to load the models only once.."""

import json, faiss, difflib, csv, os, time
import numpy as np
from sentence_transformers import SentenceTransformer
from transformers import AutoTokenizer, AutoModelForCausalLM

# === Load models
bert_model = SentenceTransformer("all-MiniLM-L6-v2")

# Switch between phi-2 (default) and phi-1_5 for faster generation
use_phi15 = False
model_id = "microsoft/phi-1_5" if use_phi15 else "microsoft/phi-2"

phi_tokenizer = AutoTokenizer.from_pretrained(model_id)
phi_model = AutoModelForCausalLM.from_pretrained(
    model_id,
    device_map="auto",
    torch_dtype="auto"
)

base_path = drive_folder
# === Warm up to prevent cold-start lag
phi_model.generate(
    **phi_tokenizer("warmup", return_tensors="pt").to(phi_model.device),
    max_new_tokens=1
)

topic = None
local_metadata = []
local_index = None
running = True

# === Load unified index + metadata from Drive
with open(os.path.join(base_path, "/content/drive/MyDrive/AI_Tutor_Big_Data/unified_faiss_metadata.json")) as f:
    metadata = json.load(f)

with open(os.path.join(base_path, "/content/drive/MyDrive/AI_Tutor_Big_Data/unified_topic_subtopic_structure.json")) as f:
    topic_structure = json.load(f)

faiss_index = faiss.read_index(os.path.join(base_path, "/content/drive/MyDrive/AI_Tutor_Big_Data/unified_faiss_index.bin"))

# === Helpers ===
def fuzzy_match(user_input, options, label="option"):
    user_input = user_input.lower()
    options_lower = {opt.lower(): opt for opt in options}
    matches = difflib.get_close_matches(user_input, list(options_lower.keys()), n=1, cutoff=0.6)
    return options_lower[matches[0]] if matches else None

def truncate_context(text, max_words=500):
    return " ".join(text.split()[:max_words])

def log_to_csv(question, answer, category, topic, subtopics):
    path = os.path.join(base_path, "chatbot_logs.csv")
    exists = os.path.exists(path)
    with open(path, "a", newline='') as f:
        writer = csv.writer(f)
        if not exists:
            writer.writerow(["Category", "Topic", "Subtopics", "Question", "Answer"])
        writer.writerow([category, topic, "; ".join(subtopics), question, answer])

# === Topic Selection Function (now global-scope safe) ===
def choose_topic(meta_filtered, category):
    global topic, local_metadata, local_index
    topics = sorted(set(m["topic"] for m in meta_filtered))
    if not topics:
        print("‚ùå No topics found.")
        return False
    print("\nüìö Available Topics:")
    for i, t in enumerate(topics, 1):
        print(f"{i}. {t}")
    user_topic = input("üîç Choose a topic: ").strip()
    topic = fuzzy_match(user_topic, topics, "topic")
    if not topic:
        print("‚ùå Topic not found.")
        return False

    filtered = [(i, m) for i, m in enumerate(meta_filtered) if m["topic"] == topic]
    texts = [m["content"] for _, m in filtered]
    local_metadata = [m for _, m in filtered]
    local_embeddings = bert_model.encode(texts, convert_to_numpy=True)
    local_index = faiss.IndexFlatL2(local_embeddings.shape[1])
    local_index.add(local_embeddings)

    print(f"\n‚úÖ Chat ready: {category} ‚Üí {topic}")
    return True

# === Main Chatbot Loop ===
while True:
    categories = sorted(set(m["category"] for m in metadata))
    print("üß† Available Categories:")
    for i, cat in enumerate(categories, 1):
        print(f"{i}. {cat}")
    user_cat = input("üìå Choose a category (or type 'all'): ").strip().lower()
    if user_cat in ['exit', 'quit']:
        print("üëã Bye!")
        break

    category = fuzzy_match(user_cat, categories + ["all"], "category")
    if not category:
        print("‚ùå Invalid category. Returning to main menu...\n")
        continue

    meta_filtered = metadata if category == "all" else [m for m in metadata if m["category"] == category]

    if not choose_topic(meta_filtered, category):
        continue

    # === Chat loop under selected topic
    while running:
        query = input("You: ").lower().strip()

        if query in ['exit', 'quit']:
            print("üëã Bye!")
            running = False
            break
        elif query in ['menu', 'main', 'change category']:
            print("üîô Returning to category menu...\n")
            break
        elif query in ['change topic', 'switch topic']:
            print("üîÑ Switching topic...\n")
            if not choose_topic(meta_filtered, category):
                break
            continue
        if not running:
            break
        # Generate Answer
        t0 = time.time()
        q_embed = bert_model.encode([query], convert_to_numpy=True)
        D, I = local_index.search(q_embed, k=3)
        top_chunks = [local_metadata[i] for i in I[0]]

        # Optional: name/entity filtering
        name_filtered = [c for c in top_chunks if any(word in query for word in c['subtopic'].lower().split(" - ")[0].split())]
        if not name_filtered:
            name_filtered = [c for c in top_chunks if any(word in query for word in c['content'].lower().split())]
        if name_filtered:
            top_chunks = name_filtered

        context = "\n\n".join([f"{c['subtopic']}:\n{c['content']}" for c in top_chunks])
        context = truncate_context(context)

        prompt = f"""You are a helpful university teaching assistant.
Answer the question using only the context below. Use faculty data from Auburn University at Montgomery only. Dont give hellucinated answers, if you dont find any just return, I'm unable to fetch the data

Context:
{context}

Question: {query}
Answer:"""

        inputs = phi_tokenizer(prompt, return_tensors="pt").to(phi_model.device)
        outputs = phi_model.generate(**inputs, max_new_tokens=200)
        raw_output = phi_tokenizer.decode(outputs[0], skip_special_tokens=True)
        answer = raw_output.split("Answer:")[-1].strip()

        print("\nüß† Answer:\n" + answer)
        print("üìö Sources:\n" + "\n".join([f"‚úîÔ∏è {c['subtopic']}" for c in top_chunks]))
        print(f"‚è±Ô∏è Time taken: {round(time.time() - t0, 2)} sec")
        print("-" * 60)

        log_to_csv(query, answer, category, topic, [c["subtopic"] for c in top_chunks])







"""##To do:

*   Valid faculty dataset and its schema.
*   Helps in increasing the accuracy of the bot.
*   Issues with the factual outputs for faculty related questions.
*   Currently, the bot is facing hallucinations


"""

!pip install gradio sentence-transformers transformers faiss-cpu

import gradio as gr

# === Setup UI state ===
categories = sorted(set(m["category"] for m in metadata))
category_topic_map = {cat: sorted(set(m["topic"] for m in metadata if m["category"] == cat)) for cat in categories}
category_topic_map["all"] = sorted(set(m["topic"] for m in metadata))

chat_history = []

def update_topics(selected_category):
    choices = category_topic_map[selected_category]
    return gr.update(choices=choices, value=choices[0])

def gradio_chatbot(category, topic, query, history):
    filtered_meta = metadata if category == "all" else [m for m in metadata if m["category"] == category]
    filtered = [(i, m) for i, m in enumerate(filtered_meta) if m["topic"] == topic]
    if not filtered:
        return history + [(query, "‚ùå No data found for selected topic.")], history

    texts = [m["content"] for _, m in filtered]
    local_metadata = [m for _, m in filtered]
    local_embeddings = bert_model.encode(texts, convert_to_numpy=True)

    if local_embeddings.shape[0] == 0:
        return history + [(query, "‚ùå No embeddings generated.")], history

    local_index = faiss.IndexFlatL2(local_embeddings.shape[1])
    local_index.add(local_embeddings)

    q_embed = bert_model.encode([query], convert_to_numpy=True)
    D, I = local_index.search(q_embed, k=2)
    top_chunks = [local_metadata[i] for i in I[0]]

    name_filtered = [c for c in top_chunks if any(word in query for word in c['subtopic'].lower().split(" - ")[0].split())]
    if not name_filtered:
        name_filtered = [c for c in top_chunks if any(word in query for word in c['content'].lower().split())]
    if name_filtered:
        top_chunks = name_filtered

    context = "\n\n".join([f"{c['subtopic']}:\n{c['content']}" for c in top_chunks])
    context = truncate_context(context)

    prompt = f"""You are a helpful university teaching assistant.
Answer the question using only the context below.

Context:
{context}

Question: {query}
Answer:"""

    inputs = phi_tokenizer(prompt, return_tensors="pt").to(phi_model.device)
    outputs = phi_model.generate(**inputs, max_new_tokens=200)
    raw_output = phi_tokenizer.decode(outputs[0], skip_special_tokens=True)
    answer = raw_output.split("Answer:")[-1].strip()

    sources = "\n".join([f"‚úîÔ∏è {c['subtopic']}" for c in top_chunks])
    response = f"{answer}\n\nüìö Sources:\n{sources}"
    history.append((query, response))

    log_to_csv(query, answer, category, topic, [c["subtopic"] for c in top_chunks])
    return history, history

with gr.Blocks() as demo:
    gr.Markdown("## üß† AI Teaching Assistant\nAsk questions related to DSA or Faculty Information")

    with gr.Row():
        category_dropdown = gr.Dropdown(label="Choose Category", choices=categories + ["all"], value="DSA")
        topic_dropdown = gr.Dropdown(label="Choose Topic", choices=category_topic_map["DSA"], value=category_topic_map["DSA"][0])

    chat = gr.Chatbot()
    msg = gr.Textbox(label="Ask your question...")
    clear = gr.Button("Clear Chat")

    category_dropdown.change(update_topics, category_dropdown, topic_dropdown)

    def clear_history():
        return [], []

    msg.submit(gradio_chatbot, [category_dropdown, topic_dropdown, msg, chat], [chat, chat])
    clear.click(clear_history, outputs=[chat, chat])

demo.launch(debug=True)



import gradio as gr

# === New Gradio-friendly backend logic ===
def load_category_topic_options():
    categories = sorted(set(m["category"] for m in metadata))
    category_topic_map = {cat: sorted(set(m["topic"] for m in metadata if m["category"] == cat)) for cat in categories}
    return categories, category_topic_map

categories, category_topic_map = load_category_topic_options()

# === Chatbot response function ===
def gradio_chatbot(category, topic, query, history=[]):
    global local_metadata, local_index

    # Filter metadata
    meta_filtered = metadata if category == "all" else [m for m in metadata if m["category"] == category]
    filtered = [(i, m) for i, m in enumerate(meta_filtered) if m["topic"] == topic]
    texts = [m["content"] for _, m in filtered]
    local_metadata = [m for _, m in filtered]

    if not texts:
        return history + [(query, "‚ùå No data found for this topic.")]

    local_embeddings = bert_model.encode(texts, convert_to_numpy=True)
    local_index = faiss.IndexFlatL2(local_embeddings.shape[1])
    local_index.add(local_embeddings)

    # Embed query and retrieve
    q_embed = bert_model.encode([query], convert_to_numpy=True)
    D, I = local_index.search(q_embed, k=5)
    top_chunks = [local_metadata[i] for i in I[0]]

    context = "\n\n".join([f"{c['subtopic']}:\n{c['content']}" for c in top_chunks])
    context = truncate_context(context)

    prompt = f"""You are a helpful university teaching assistant.
Answer the question using only the context below. Use faculty data from Auburn University at Montgomery only.
If you don't find any relevant data, say: 'I'm unable to fetch the data.'

Context:
{context}

Question: {query}
Answer:"""

    inputs = phi_tokenizer(prompt, return_tensors="pt").to(phi_model.device)
    outputs = phi_model.generate(**inputs, max_new_tokens=300)
    raw_output = phi_tokenizer.decode(outputs[0], skip_special_tokens=True)
    answer = raw_output.split("Answer:")[-1].strip()

    log_to_csv(query, answer, category, topic, [c["subtopic"] for c in top_chunks])
    return history + [(query, answer)]

# === Gradio interface ===
def update_topics(selected_category):
    return gr.update(choices=category_topic_map[selected_category], value=category_topic_map[selected_category][0])

with gr.Blocks() as demo:
    gr.Markdown("""# üß† AI Teaching Assistant\nAsk questions related to DSA or Faculty Information""")

    with gr.Row():
        category_dropdown = gr.Dropdown(label="Choose Category", choices=categories, value=categories[0])
        topic_dropdown = gr.Dropdown(label="Choose Topic", choices=category_topic_map[categories[0]])

    chatbot = gr.Chatbot()
    msg = gr.Textbox(label="Ask your question")
    clear = gr.Button("Clear Chat")

    category_dropdown.change(update_topics, inputs=category_dropdown, outputs=topic_dropdown)
    msg.submit(gradio_chatbot, inputs=[category_dropdown, topic_dropdown, msg, chatbot], outputs=chatbot)
    clear.click(lambda: [], None, chatbot)

    gr.Markdown("""---\n‚öôÔ∏è Built with MiniLM + Phi-2 + FAISS + Gradio""")

demo.launch(share=True)
print([m.get("name") for m in metadata if "computer science" in m.get("department", "").lower()])







import gradio as gr

# === New Gradio-friendly backend logic ===
def load_category_topic_options():
    categories = sorted(set(m["category"] for m in metadata))
    category_topic_map = {cat: sorted(set(m["topic"] for m in metadata if m["category"] == cat)) for cat in categories}
    return categories, category_topic_map

categories, category_topic_map = load_category_topic_options()

# === Chatbot response function ===
def gradio_chatbot(category, topic, query, history=[]):
    global local_metadata, local_index

    # Filter metadata
    meta_filtered = metadata if category == "all" else [m for m in metadata if m["category"] == category]
    filtered = [(i, m) for i, m in enumerate(meta_filtered) if m["topic"] == topic]
    texts = [m["content"] for _, m in filtered]
    local_metadata = [m for _, m in filtered]

    if not texts:
        return history + [(query, "‚ùå No data found for this topic.")]

    # === Check for department + research query
    if "research" in query.lower() and "faculty" in query.lower():
        dept_keywords = query.lower().split()
        results = []
        for m in local_metadata:
            dept = m.get("department", "").lower()
            research = m.get("research_areas", "").strip()
            name = m.get("name", "")
            if any(word in dept for word in dept_keywords) and research:
                web = m.get("web_link", "").strip()
                entry = f"{name}: {research}"
                if web:
                    entry += f"\nüîó {web}"
                results.append(entry)
        if results:
            response = "Here are the research interests of faculty:\n\n" + "\n\n".join("‚Ä¢ " + r for r in results)
            return history + [(query, response)]
        else:
            return history + [(query, "I'm unable to fetch any research data for the specified department.")]


    local_embeddings = bert_model.encode(texts, convert_to_numpy=True)
    local_index = faiss.IndexFlatL2(local_embeddings.shape[1])
    local_index.add(local_embeddings)

    # Embed query and retrieve
    q_embed = bert_model.encode([query], convert_to_numpy=True)
    D, I = local_index.search(q_embed, k=5)
    top_chunks = [local_metadata[i] for i in I[0]]

    context = "\n\n".join([f"{c['subtopic']}:\n{c['content']}" for c in top_chunks])
    context = truncate_context(context)

    prompt = f"""You are a helpful university teaching assistant.
Answer the question using only the context below. Use faculty data from Auburn University at Montgomery only.
If you don't find any relevant data, say: 'I'm unable to fetch the data.'

Context:
{context}

Question: {query}
Answer:"""

    inputs = phi_tokenizer(prompt, return_tensors="pt").to(phi_model.device)
    outputs = phi_model.generate(**inputs, max_new_tokens=300)
    raw_output = phi_tokenizer.decode(outputs[0], skip_special_tokens=True)
    answer = raw_output.split("Answer:")[-1].strip()

    log_to_csv(query, answer, category, topic, [c["subtopic"] for c in top_chunks])
    return history + [(query, answer)]

# === Gradio interface ===
def update_topics(selected_category):
    return gr.update(choices=category_topic_map[selected_category], value=category_topic_map[selected_category][0])

with gr.Blocks() as demo:
    gr.Markdown("""# üß† AI Teaching Assistant\nAsk questions related to DSA or Faculty Information""")

    with gr.Row():
        category_dropdown = gr.Dropdown(label="Choose Category", choices=categories, value=categories[0])
        topic_dropdown = gr.Dropdown(label="Choose Topic", choices=category_topic_map[categories[0]])

    chatbot = gr.Chatbot()
    msg = gr.Textbox(label="Ask your question")
    clear = gr.Button("Clear Chat")

    category_dropdown.change(update_topics, inputs=category_dropdown, outputs=topic_dropdown)
    msg.submit(gradio_chatbot, inputs=[category_dropdown, topic_dropdown, msg, chatbot], outputs=chatbot)
    clear.click(lambda: [], None, chatbot)

    gr.Markdown("""---\n‚öôÔ∏è Built with MiniLM + Phi-2 + FAISS + Gradio""")

demo.launch(share=True)
# print([m.get("name") for m in metadata if "computer science" in m.get("department", "").lower()])

