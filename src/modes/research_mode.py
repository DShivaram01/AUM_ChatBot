# -*- coding: utf-8 -*-
"""research_2.0_frozen.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/18pSBy9k8-YhbwaEwHfkhWBpGsNWHZZk5

# Set the runtime to T4
"""

#Install only once
!pip -q install sentence-transformers faiss-cpu transformers accelerate gradio rank-bm25

import os, json, time, re
from typing import List, Dict, Any, Optional

import numpy as np
import faiss
import torch
from sentence_transformers import SentenceTransformer, CrossEncoder
from transformers import AutoTokenizer, AutoModelForCausalLM
import gradio as gr

from google.colab import drive
drive.mount('/content/drive')

#Set the DATA_FILE path for cos_data.jsonl
DATA_FILE        = "/content/drive/MyDrive/COS_dataset/cos_data.jsonl"     # your JSONL (one JSON per line)
#Set the OUT_DIR path to store the embeddings
OUT_DIR          = "/content/drive/MyDrive/COS_dataset/emb_store"; os.makedirs(OUT_DIR, exist_ok=True)

EMB_PATH    = f"{OUT_DIR}/embeddings.npy"
ID_PATH     = f"{OUT_DIR}/ids.json"
META_PATH   = f"{OUT_DIR}/metadata.json"
TEXTS_PATH  = f"{OUT_DIR}/texts.json"
FAISS_PATH  = f"{OUT_DIR}/faiss_ip.index"

EMBED_MODEL_ID = "sentence-transformers/all-MiniLM-L6-v2"
RERANKER_ID    = "cross-encoder/ms-marco-MiniLM-L-6-v2"
PHI3_ID        = "microsoft/phi-2"
DEVICE         = "cuda" if torch.cuda.is_available() else "cpu"

# =========================================================
# 2) Load JSONL exactly as given (no extra mapping)
#    Expect keys: id, text, metadata{ title, lead_presenters, other_authors,
#                                     mentor, department, category, year, keywords, ...}
# =========================================================
def load_jsonl(path: str) -> List[Dict[str, Any]]:
    rows = []
    with open(path, "r", encoding="utf-8") as f:
        for line in f:
            line = line.strip()
            if not line:
                continue
            rows.append(json.loads(line))
    return rows

rows = load_jsonl(DATA_FILE)
print(f"Loaded {len(rows)} rows")

# Build arrays weâ€™ll use everywhere (IDs, TEXTS, METADATA)
IDS    = [r["id"] for r in rows]
TEXTS  = [r["text"] for r in rows]
META   = [r["metadata"] for r in rows]

# Quick peek (first row)
print("\nSample row (keys):")
print("id:", IDS[0])
print("metadata keys:", list(META[0].keys())[:10])
print("text first 200 chars:\n", TEXTS[0][:200], "...")

# =========================================================
# 3) Embeddings (cosine) + FAISS IndexFlatIP
# =========================================================
embedder = SentenceTransformer(EMBED_MODEL_ID, device=DEVICE)

def build_store():
    print("\n[Build] Encoding texts for embeddings (cosine)...")
    EMB = embedder.encode(TEXTS, batch_size=64, convert_to_numpy=True, normalize_embeddings=True)
    print("[Build] Building FAISS (IndexFlatIP + IDMap)...")
    index = faiss.IndexFlatIP(EMB.shape[1])
    index = faiss.IndexIDMap2(index)
    index.add_with_ids(EMB, np.arange(len(IDS)).astype(np.int64))

    np.save(EMB_PATH, EMB)
    with open(ID_PATH, "w") as f: json.dump(IDS, f)
    with open(META_PATH, "w") as f: json.dump(META, f, indent=2)
    with open(TEXTS_PATH, "w") as f: json.dump(TEXTS, f)
    faiss.write_index(index, FAISS_PATH)
    print("âœ… Store built & saved:", index.ntotal, "vectors")

def load_store():
    index = faiss.read_index(FAISS_PATH)
    EMB = np.load(EMB_PATH)
    with open(ID_PATH) as f: id_list = json.load(f)
    with open(META_PATH) as f: metadata = json.load(f)
    with open(TEXTS_PATH) as f: texts = json.load(f)
    return index, EMB, id_list, metadata, texts

if not (os.path.exists(FAISS_PATH) and os.path.exists(EMB_PATH) and os.path.exists(ID_PATH) and os.path.exists(META_PATH) and os.path.exists(TEXTS_PATH)):
    build_store()

index, EMB, ID_LIST, META_LIST, TEXTS_LIST = load_store()
print("Index size:", index.ntotal, "| Device:", DEVICE)

"""# Name Normalization

"""

from collections import defaultdict
import re

# --- Generic cleaner for texty fields (dept, category, keywords) ---
def norm_text(s: str) -> str:
    if not s:
        return ""
    s = s.lower().strip()
    # collapse multiple spaces
    s = re.sub(r"\s+", " ", s)
    return s

# --- Name-specific cleaner (lead_presenters, authors, mentor) ---
def norm_name(name: str) -> str:
    if not name:
        return ""
    name = name.lower()
    # remove typical "Dr", "Prof" prefixes in various forms
    name = re.sub(r"\b(dr\.?|prof\.?|professor)\b", "", name)
    # remove dots in initials: "O. Kursun" -> "o kursun"
    name = name.replace(".", " ")
    # collapse multiple spaces
    name = re.sub(r"\s+", " ", name)
    return name.strip()

# People-based indexes
INDEX_LEAD_PRESENTERS = defaultdict(list)   # norm_name(lead_presenter) -> [row_idx,...]
# INDEX_OTHER_AUTHORS   = defaultdict(list)   # norm_name(author)         -> [row_idx,...]
INDEX_MENTOR          = defaultdict(list)   # norm_name(mentor)        -> [row_idx,...]

# Non-people fields
INDEX_DEPARTMENT      = defaultdict(list)   # norm_text(dept)          -> [row_idx,...]
INDEX_CATEGORY        = defaultdict(list)   # norm_text(category)      -> [row_idx,...]
INDEX_YEAR            = defaultdict(list)   # "2024"                   -> [row_idx,...]
INDEX_KEYWORD         = defaultdict(list)   # norm_text(keyword)       -> [row_idx,...]

# Optional: canonical lists (useful for later detection / dropdowns)
ALL_LEAD_PRESENTERS = set()
# ALL_OTHER_AUTHORS   = set()
ALL_MENTORS         = set()
ALL_DEPARTMENTS     = set()
ALL_CATEGORIES      = set()
ALL_YEARS           = set()
ALL_KEYWORDS        = set()

from rank_bm25 import BM25Okapi


def build_bm25_corpus():
    """
    Build BM25 over *metadata only* so that name / year / dept queries
    hit exactly the structured fields and not random mentions in text.
    """
    corpus_tokens = []

    for meta, full_text in zip(META_LIST, TEXTS_LIST):
        title   = meta.get("title", "") or ""
        mentor  = meta.get("mentor", "") or ""
        leads   = " ".join(meta.get("lead_presenters", []) or [])
        dept    = meta.get("department", "") or ""
        cat     = meta.get("category", "") or ""
        year    = str(meta.get("year", "") or "")
        keys    = " ".join(meta.get("keywords", []) or [])

        # NOTE: we intentionally drop full_text from BM25 now.
        combined = " ".join([
            title,
            mentor,
            leads,
            dept,
            cat,
            year,
            keys,
        ])

        tokens = combined.lower().split()
        corpus_tokens.append(tokens)

    return BM25Okapi(corpus_tokens)


print("\n[Build] BM25 corpus...")
bm25 = build_bm25_corpus()
print("[Build] BM25 ready.")

for idx, meta in enumerate(META_LIST):
    # --- lead_presenters: list of names ---
    for lp in meta.get("lead_presenters", []) or []:
        lp = lp.strip()
        if not lp:
            continue
        ALL_LEAD_PRESENTERS.add(lp)
        lp_norm = norm_name(lp)
        if lp_norm:
            INDEX_LEAD_PRESENTERS[lp_norm].append(idx)

    # --- other_authors: list of names ---
    # for au in meta.get("other_authors", []) or []:
    #     au = au.strip()
    #     if not au:
    #         continue
    #     ALL_OTHER_AUTHORS.add(au)
    #     au_norm = norm_name(au)
    #     if au_norm:
    #         INDEX_OTHER_AUTHORS[au_norm].append(idx)

    # --- mentor: single name ---
    mentor = (meta.get("mentor") or "").strip()
    if mentor:
        ALL_MENTORS.add(mentor)
        m_norm = norm_name(mentor)
        if m_norm:
            INDEX_MENTOR[m_norm].append(idx)

    # --- department ---
    dept = (meta.get("department") or "").strip()
    if dept:
        ALL_DEPARTMENTS.add(dept)
        d_norm = norm_text(dept)
        if d_norm:
            INDEX_DEPARTMENT[d_norm].append(idx)

    # --- category ---
    cat = (meta.get("category") or "").strip()
    if cat:
        ALL_CATEGORIES.add(cat)
        c_norm = norm_text(cat)
        if c_norm:
            INDEX_CATEGORY[c_norm].append(idx)

    # --- year ---
    year = meta.get("year")
    if year is not None:
        year_str = str(year).strip()
        if year_str:
            ALL_YEARS.add(year_str)
            INDEX_YEAR[year_str].append(idx)

    # --- keywords: list of strings ---
    for kw in meta.get("keywords", []) or []:
        kw = kw.strip()
        if not kw:
            continue
        ALL_KEYWORDS.add(kw)
        kw_norm = norm_text(kw)
        if kw_norm:
            INDEX_KEYWORD[kw_norm].append(idx)

# =========================================================
# 4) Reranker + Phi-2
# =========================================================
reranker = CrossEncoder(RERANKER_ID, device=DEVICE)

phi_tok = AutoTokenizer.from_pretrained(PHI3_ID)
phi_model = AutoModelForCausalLM.from_pretrained(
    PHI3_ID, device_map="auto",
    torch_dtype=(torch.float16 if DEVICE=="cuda" else torch.float32)
)
# warmup
_ = phi_model.generate(**phi_tok("hi", return_tensors="pt").to(phi_model.device), max_new_tokens=1)
print("Models ready.")

# =========================================================
# 5) Helpers (basic, no try/except)
# =========================================================
def extract_abstract(full_text: str) -> str:
    # Split on the first "Abstract:" and take the rest
    parts = full_text.split("Abstract:", 1)
    if len(parts) == 2:
        return parts[1].strip()
    return full_text.strip()

def print_hit(rank: int, meta: Dict[str, Any], vec: float, rr: Optional[float]):
    title = meta.get("title", "")
    year  = meta.get("year", "")
    dept  = meta.get("department", "")
    cats  = meta.get("category", "")
    print(f"\n--- Hit {rank} ---")
    print(f"Title: {title}")
    print(f"Year: {year} | Dept: {dept} | Category: {cats}")
    print(f"VecScore: {vec:.4f} | Rerank: {0.0 if rr is None else float(rr):.4f}")

def retrieve_layered(query: str,
                     top_k_bm25: int = 50,
                     top_k_vec: int = 30,
                     max_for_rerank: int = 30) -> List[Dict[str, Any]]:
    """
    Layered retrieve:
    1) Try BM25 over metadata-only corpus.
    2) If BM25 produces candidates -> rerank them with CrossEncoder (no FAISS).
    3) If BM25 is empty -> fallback to FAISS semantic search + rerank.
    """

    print(f"\nðŸ§  Query (layered): {query}")

    # ---------------------------
    # 1) BM25 keyword search (metadata)
    # ---------------------------
    t_bm0 = time.time()
    bm25_idxs, bm25_scores = bm25_search(query, top_k=top_k_bm25)
    print(f"BM25 time: {(time.time()-t_bm0)*1000:.1f} ms")

    bm25_candidates = {}
    for idx in bm25_idxs:
        score = float(bm25_scores[idx])
        if score > 0.0:
            bm25_candidates[int(idx)] = score

    cands: List[Dict[str, Any]] = []

    if bm25_candidates:
        # ---- Use BM25-only path ----
        max_bm = max(bm25_candidates.values()) if bm25_candidates else 1.0

        for idx, score in bm25_candidates.items():
            meta = META_LIST[idx]
            text = TEXTS_LIST[idx]
            cands.append({
                "idx": int(idx),
                "vec": 0.0,                  # not used in this path
                "bm25": float(score),
                "combined": float(score / max_bm),
                "rerank": None,
                "meta": meta,
                "text": text,
            })

        # sort by BM25 score
        cands.sort(key=lambda x: x["bm25"], reverse=True)

        print("\nðŸ“¥ Candidates from BM25 (before rerank):")
        for i, c in enumerate(cands[:5], 1):
            print(f"{i}. bm25={c['bm25']:.3f}")
            print("   Title:", c["meta"].get("title", "NO TITLE"))
            print("   Mentor:", c["meta"].get("mentor", ""))
            print("   Year:", c["meta"].get("year", ""))

    else:
        # ---------------------------
        # 2) Fallback: FAISS semantic search
        # ---------------------------
        print("BM25 produced no candidates. Falling back to FAISS.")
        t0 = time.time()
        q = embedder.encode([query], convert_to_numpy=True, normalize_embeddings=True)
        D, I = index.search(q, top_k_vec)
        print(f"FAISS time: {(time.time()-t0)*1000:.1f} ms")

        vec_candidates = {}
        for rank, idx in enumerate(I[0]):
            if idx == -1:
                continue
            vec_candidates[int(idx)] = float(D[0][rank])

        if not vec_candidates:
            print("No candidates from FAISS either.")
            return []

        max_vec = max(vec_candidates.values()) if vec_candidates else 1.0

        for idx, vscore in vec_candidates.items():
            meta = META_LIST[idx]
            text = TEXTS_LIST[idx]
            cands.append({
                "idx": int(idx),
                "vec": float(vscore),
                "bm25": 0.0,
                "combined": float(vscore / max_vec),
                "rerank": None,
                "meta": meta,
                "text": text,
            })

        print("\nðŸ“¥ Candidates from FAISS (before rerank):")
        for i, c in enumerate(cands[:5], 1):
            print(f"{i}. vec={c['vec']:.3f}")
            print("   Title:", c["meta"].get("title", "NO TITLE"))
            print("   Mentor:", c["meta"].get("mentor", ""))
            print("   Year:", c["meta"].get("year", ""))

    # ---------------------------
    # 3) CrossEncoder rerank on current candidates
    # ---------------------------
    keep = min(len(cands), max_for_rerank)
    if keep > 0:
        t2 = time.time()
        pairs = [(query, cands[i]["text"]) for i in range(keep)]
        scores = reranker.predict(pairs)
        for i, s in enumerate(scores):
            cands[i]["rerank"] = float(s)
        cands[:keep] = sorted(cands[:keep], key=lambda x: x["rerank"], reverse=True)
        print(f"\nðŸ” Rerank time: {(time.time()-t2)*1000:.1f} ms")
        print("\nðŸ“¦ Top (after rerank):")
        for i, c in enumerate(cands[:5], 1):
            print(f"{i}. rerank={c['rerank']:.3f}")
            print("   Title:", c["meta"].get("title", "NO TITLE"))
            print("   Mentor:", c["meta"].get("mentor", ""))
            print("   Year:", c["meta"].get("year", ""))

    return cands

def bm25_search(query: str, top_k: int = 50):
    """
    BM25 keyword search over combined metadata + text.
    Returns:
        top_idxs: numpy array of indices (int)
        scores:   numpy array of BM25 scores for all docs
    """
    q_tokens = query.lower().split()
    scores = np.array(bm25.get_scores(q_tokens), dtype=np.float32)

    if top_k >= len(scores):
        top_idxs = np.argsort(scores)[::-1]
    else:
        top_idxs = np.argsort(scores)[::-1][:top_k]

    return top_idxs, scores


def build_summary_prompt(meta: dict, abstract: str) -> str:
    title   = meta.get("title","")
    leads   = ", ".join(meta.get("lead_presenters", []))
    others  = ", ".join(meta.get("other_authors", []))
    mentor  = meta.get("mentor","")
    dept    = meta.get("department","")
    cat     = meta.get("category","")
    year    = meta.get("year","XXXX")
    keys    = ", ".join(meta.get("keywords", []))

    # Plain string prompt; model sees context in a fenced block and is told what to output.
    return f"""Task: Write a concise research summary.

Rules:
- Output exactly ONE paragraph of 5â€“6 sentences.
- Do NOT include headings or bullet points.
- Do NOT repeat the input or metadata.
- Use only the given facts; no speculation.

Context (for reference only):
Title: {title}
Year: {year}
Lead Presenters: {leads}
Other Authors: {others}
Mentor: {mentor}
Department: {dept}
Category: {cat}
Keywords: {keys}




Summary:"""

import re

META_MARKERS = ("Title:", "Year:", "Lead Presenters:", "Other Authors:", "Mentor:", "Department:", "Category:", "Keywords:", "Abstract:")

def clean_summary(text: str) -> str:
    # Keep only content after the "Summary:" anchor (if present)
    if "Summary:" in text:
        text = text.split("Summary:", 1)[1]

    # Stop before any metadata markers if the model starts echoing them
    for m in META_MARKERS:
        if m in text:
            text = text.split(m, 1)[0]

    # Keep only the first paragraph
    text = text.strip().split("\n\n", 1)[0].strip()

    # Remove duplicate sentences while preserving order (simple de-dupe)
    sents = re.split(r'(?<=[.!?])\s+', text)
    seen, dedup = set(), []
    for s in sents:
        ss = s.strip()
        if ss and ss.lower() not in seen:
            seen.add(ss.lower())
            dedup.append(ss)
    return " ".join(dedup).strip()

def format_meta_details(meta: dict) -> str:
    title  = meta.get("title", "")
    leads  = ", ".join(meta.get("lead_presenters", []))
    others = ", ".join(meta.get("other_authors", []))
    mentor = meta.get("mentor", "")
    dept   = meta.get("department", "")
    cat    = meta.get("category", "")
    year   = meta.get("year", "XXXX")
    keys   = ", ".join(meta.get("keywords", []))

    lines = [
        f"Title: {title}",
        f"Lead Presenters: {leads}" if leads else "Lead Presenters: -",
        f"Other Authors: {others}" if others else "Other Authors: -",
        f"Mentor: {mentor}" if mentor else "Mentor: -",
        f"Department: {dept}" if dept else "Department: -",
        f"Category: {cat}" if cat else "Category: -",
        f"Year: {year}",
        f"Keywords: {keys}" if keys else "Keywords: -",
    ]
    return "\n".join(lines)

def summarize_best(best: dict, max_new_tokens: int = 140) -> tuple[str, str]:
    abstract = extract_abstract(best["text"])
    prompt = build_summary_prompt(best["meta"], abstract)

    inputs = phi_tok(prompt, return_tensors="pt", truncation=True).to(phi_model.device)
    out = phi_model.generate(
        **inputs,
        max_new_tokens=max_new_tokens,
        do_sample=False,
        no_repeat_ngram_size=4,
        repetition_penalty=1.12,
        eos_token_id=phi_tok.eos_token_id
    )
    raw = phi_tok.decode(out[0], skip_special_tokens=True)
    summary = clean_summary(raw)           # if using cleaner
    details = format_meta_details(best["meta"])
    return summary, details

"""Infering modes(list/single) from reranked candidates"""

def infer_mode_from_rerank_candidates(cands) -> str:
    """
    Decide 'single' vs 'list' based on CrossEncoder rerank scores.

    Logic:
    - Use the rerank scores (more semantic, see full text).
    - If we have no rerank scores (e.g., reranker skipped) â†’ treat as 'single'
      so the user gets a useful summary instead of an empty list.
    - If exactly one candidate has a rerank score â†’ 'single'.
    - Otherwise, look at how sharply the top score stands out:
        - If top1 is clearly stronger than top2 (ratio), treat as 'single'.
        - Else, treat as 'list'.
    """

    # Collect all candidates that have a valid rerank score
    scores = sorted(
        [c["rerank"] for c in cands if c.get("rerank") is not None],
        reverse=True
    )

    if not scores:
        # No rerank information (should be rare) â†’ default to single + summary
        return "single"

    if len(scores) == 1:
        return "single"

    top1 = scores[0]
    top2 = scores[1]

    # If top1 is non-positive or tiny, treat everything as weak â†’ list
    if top1 <= 0:
        return "list"

    # If top2 is <= 0, but top1 > 0 â†’ very likely a single strong hit
    if top2 <= 0:
        return "single"

    ratio = top1 / top2

    # Heuristic:
    # - If top1 is ~1.5x or more larger than top2 â†’ "single"
    #   (you can tune 1.5 up/down after observing scores)
    if ratio >= 1.5:
        return "single"

    # Otherwise, rerank scores are relatively flat â†’ list
    return "list"

# =========================================================
# 7) Gradio UI (uses layered retrieval + BM25-based mode)
# =========================================================
def run_query(query: str):
    # Layered retrieval:
    # 1) BM25 over metadata-only corpus
    # 2) If BM25 empty â†’ FAISS fallback
    topk=len(IDS)
    cands = retrieve_layered(
        query,
        top_k_bm25=min(len(IDS), 500),   # you can tune this cap
        top_k_vec=int(topk),             # used only in FAISS fallback
        max_for_rerank=min(len(IDS), 50)
    )
    if not cands:
      combined_update = gr.update(value="No results.", visible=True)
      return "No results.", []

    # Decide whether to summarize one project or show a list,
    # based purely on BM25 score behaviour.
    mode = infer_mode_from_rerank_candidates(cands)
    print(f"Mode: {mode}")


    # Build table rows (same for both modes)
    rows = []
    max_rows = 50
    for i, c in enumerate(cands[:max_rows], 1):
        m = c["meta"]
        rows.append([
            i,
            m.get("title", ""),
            m.get("year", ""),
            m.get("mentor",""),
            m.get("department",""),
            round(c.get("rerank", 0.0), 4)
        ])

    if mode == "list":
        # LIST MODE: user likely asked for "papers/projects under X", "projects in 2025", etc.
        combined = gr.update(value="", visible=False)
        return combined, rows

    else:
        # SINGLE MODE: BM25 strongly prefers one project â†’ summarize that one
        best = cands[0]
        summary, details = summarize_best(best)
        combined = f"{summary}\n\n---\n{details}"
        return combined, rows

with gr.Blocks() as demo:
    q = gr.Textbox(label="Query", value="Deep Learning Approaches for Protein Homology Detection")

    btn = gr.Button("Search")

    out_combined = gr.Textbox(label="Summary or List Info", lines=5)
    out_table = gr.Dataframe(
        headers=["Rank","Title","Year","Mentor","Department","RerankScore"],
        row_count=(0, "dynamic"),
        wrap=True
    )
    btn.click(run_query, [q], [out_combined, out_table])

demo.launch(debug=True, share=False)

# # =========================================================
# # 7) Gradio UI â€“ same look as Version 1, no sliders/tables
# # =========================================================
# with gr.Blocks() as demo:
#     gr.Markdown("## ðŸ§  AI Teaching Assistant\nAsk about COS research projects")

#     chatbox = gr.Chatbot(type="messages", label="Chat History")
#     query_input = gr.Textbox(label="Ask your question...")
#     clear = gr.Button("Clear Chat")

#     # Keep only one state: full chat history
#     history_state = gr.State([])

#     # When user presses Enter:
#     #  - call research_chat_handler
#     #  - update chatbox + history_state
#     query_input.submit(
#         research_chat_handler,
#         inputs=[query_input, history_state],
#         outputs=[chatbox, history_state],
#     )

#     # Clear the textbox after submit
#     query_input.submit(lambda _: "", inputs=query_input, outputs=query_input)

#     # Clear button: reset chat + history
#     clear.click(lambda: ([], []), outputs=[chatbox, history_state])

# demo.launch(debug=True, share=False)







