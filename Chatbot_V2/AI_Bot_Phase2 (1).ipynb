{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "rnXwqajRw2g5",
        "outputId": "a0dc3d28-6467-480d-e67f-eed176844efa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (4.1.0)\n",
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.11.0.post1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (5.0 kB)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.53.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (2.6.0+cu124)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.15.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (0.33.4)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (11.2.1)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.14.1)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (25.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.3.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.1.5)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2025.7.14)\n",
            "Downloading faiss_cpu-1.11.0.post1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (31.3 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m31.3/31.3 MB\u001b[0m \u001b[31m70.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m109.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m92.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m58.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m101.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, faiss-cpu, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed faiss-cpu-1.11.0.post1 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "nvidia"
                ]
              },
              "id": "00c421c516284d39b120084bdb067078"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "pip install sentence-transformers faiss-cpu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i--p2awfw7AX",
        "outputId": "08906c59-8bbc-470b-dfac-020bf1ed3e06"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UKEgfjJQw9j7",
        "outputId": "444829cf-16b5-40a6-9f3f-1277cf8a4307"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Data converted and index saved.\n"
          ]
        }
      ],
      "source": [
        "# === SECTION 1: DATA CONVERSION & EMBEDDING PREP ===\n",
        "# Convert structured JSON faculty data into content chunks for embedding\n",
        "\n",
        "import json, os\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import numpy as np\n",
        "import faiss\n",
        "\n",
        "# === Config ===\n",
        "DATA_FILE = \"/content/drive/MyDrive/AI_Tutor_Phase2/faculty_data.json\"\n",
        "EMBEDDING_OUTPUT_PATH = \"/content/drive/MyDrive/AI_Tutor_Phase2/faculty_faiss_index.bin\"\n",
        "METADATA_OUTPUT_PATH = \"/content/drive/MyDrive/AI_Tutor_Phase2/faculty_metadata.json\"\n",
        "\n",
        "# === Load Data ===\n",
        "with open(DATA_FILE, \"r\") as f:\n",
        "    raw_data = json.load(f)\n",
        "\n",
        "# === Load Embedding Model ===\n",
        "bert_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "\n",
        "# === Process Data ===\n",
        "metadata = []\n",
        "texts = []\n",
        "\n",
        "for prof in raw_data:\n",
        "    try:\n",
        "        content = f\"\"\"\n",
        "Name: {prof['name']}\n",
        "Designation: {prof['designation']}\n",
        "Department: {prof['department']}\n",
        "Email: {prof['email']}\n",
        "Courses Taught: {', '.join(prof.get('courses_taught', []))}\n",
        "Recent Publications: {', '.join(prof.get('recent_publications', []))}\n",
        "Ongoing Projects: {prof.get('ongoing_projects', '')}\n",
        "Research Interests: {prof.get('research_interests', '')}\n",
        "Research Keywords: {', '.join(prof.get('research_keywords', []))}\n",
        "\"\"\".strip()\n",
        "\n",
        "        texts.append(content)\n",
        "        metadata.append({\n",
        "            \"name\": prof[\"name\"],\n",
        "            \"designation\": prof[\"designation\"],\n",
        "            \"department\": prof[\"department\"],\n",
        "            \"email\": prof[\"email\"],\n",
        "            \"courses_taught\": prof.get(\"courses_taught\", []),\n",
        "            \"recent_publications\": prof.get(\"recent_publications\", []),\n",
        "            \"ongoing_projects\": prof.get(\"ongoing_projects\", \"\"),\n",
        "            \"research_interests\": prof.get(\"research_interests\", \"\"),\n",
        "            \"research_keywords\": prof.get(\"research_keywords\", []),\n",
        "            \"content\": content\n",
        "        })\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ Error processing entry: {prof.get('name', 'Unknown')} - {e}\")\n",
        "\n",
        "# === Encode & Save ===\n",
        "embeddings = bert_model.encode(texts, convert_to_numpy=True)\n",
        "index = faiss.IndexFlatL2(embeddings.shape[1])\n",
        "index.add(embeddings)\n",
        "\n",
        "faiss.write_index(index, EMBEDDING_OUTPUT_PATH)\n",
        "with open(METADATA_OUTPUT_PATH, \"w\") as f:\n",
        "    json.dump(metadata, f, indent=2)\n",
        "\n",
        "print(\"âœ… Data converted and index saved.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SJaC84xGNW01"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84,
          "referenced_widgets": [
            "6df12d9e0780469e8411a1e8c58550ff",
            "64a67343577f47939324a2fede432bd4",
            "4c8f35d277fc484f8adc7e97426993d5",
            "b76c9ed1c9854573ba56d948ae1cd8dc",
            "fbc58c600c8e400d855a73833dc1602d",
            "633b872f97e646ac8489f783ad18470a",
            "d36ce85fe980450fae66df86eb55d746",
            "abc8d5252cf1487ea8458ef25dc5412c",
            "740ba14150d341a5a28876baa089ad3e",
            "dba1de8cf29c46e58f3b6a9c5073c855",
            "8522b95c534e414a930bca9080252111"
          ]
        },
        "id": "HCTLMINHw9me",
        "outputId": "16f41599-b9d5-4c32-9adb-40bb9afacb84"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6df12d9e0780469e8411a1e8c58550ff"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Models and Index Loaded Successfully\n"
          ]
        }
      ],
      "source": [
        "# === SECTION 2: Model Loading and Initialization ===\n",
        "\n",
        "import os\n",
        "import json\n",
        "import faiss\n",
        "import torch\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "# === Paths ===\n",
        "drive_folder = \"/content/drive/MyDrive/AI_Tutor_Phase2\"\n",
        "metadata_path = os.path.join(drive_folder, \"faculty_metadata.json\")\n",
        "index_path = os.path.join(drive_folder, \"faculty_faiss_index.bin\")\n",
        "\n",
        "# === Load Metadata ===\n",
        "with open(metadata_path, \"r\") as f:\n",
        "    metadata = json.load(f)\n",
        "\n",
        "# === Load FAISS Index ===\n",
        "faiss_index = faiss.read_index(index_path)\n",
        "\n",
        "# === Load Sentence-BERT model for query embedding ===\n",
        "bert_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "\n",
        "# === Load Phi model for generation ===\n",
        "use_phi15 = False  # Set to True to use smaller, faster phi-1_5\n",
        "phi_model_id = \"microsoft/phi-1_5\" if use_phi15 else \"microsoft/phi-2\"\n",
        "phi_tokenizer = AutoTokenizer.from_pretrained(phi_model_id)\n",
        "phi_model = AutoModelForCausalLM.from_pretrained(\n",
        "    phi_model_id,\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=\"auto\"\n",
        ")\n",
        "\n",
        "# === Warmup Phi model to prevent cold-start lag ===\n",
        "phi_model.generate(\n",
        "    **phi_tokenizer(\"warmup\", return_tensors=\"pt\").to(phi_model.device),\n",
        "    max_new_tokens=1\n",
        ")\n",
        "\n",
        "print(\"âœ… Models and Index Loaded Successfully\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3TLtjLK-9BHb"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O158-a_x90KH"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "VGum72TE90Nm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 767
        },
        "outputId": "48c6adc2-b01b-46d0-e1d2-ddfd4d846fb8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-5-2712315306.py:138: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
            "  chatbox = gr.Chatbot()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://be8501ffd9ef8bee64.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://be8501ffd9ef8bee64.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ğŸš¦ Mode Selected: Exam QA Mode\n",
            "\n",
            "ğŸ§  Incoming Exam QA Query: Generate QA for subject: Machine Learning, topic: Supervised Algorithms\n",
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://be8501ffd9ef8bee64.gradio.live\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "import gradio as gr\n",
        "\n",
        "# === Util: Reranking Function ===\n",
        "def rerank_results(query, chunks):\n",
        "    priority = []\n",
        "    query_lower = query.lower()\n",
        "\n",
        "    for c in chunks:\n",
        "        score = 0\n",
        "        dept = c.get(\"department\", \"\").lower()\n",
        "        r_keywords = [kw.lower() for kw in c.get(\"research_keywords\", [])]\n",
        "        r_interests = c.get(\"research_interests\", \"\").lower()\n",
        "        projects = c.get(\"ongoing_projects\", \"\").lower()\n",
        "        courses = [c_name.lower() for c_name in c.get(\"courses_taught\", []) if isinstance(c_name, str)]\n",
        "\n",
        "        if any(word in query_lower for word in dept.split()):\n",
        "            score += 1\n",
        "        if any(kw in query_lower for kw in r_keywords):\n",
        "            score += 1\n",
        "        if r_interests and r_interests in query_lower:\n",
        "            score += 1\n",
        "        if any(cname in query_lower for cname in courses):\n",
        "            score += 0.5\n",
        "        if any(word in query_lower for word in projects.split()):\n",
        "            score += 0.5\n",
        "\n",
        "        priority.append((score, c))\n",
        "\n",
        "    priority.sort(reverse=True, key=lambda x: x[0])\n",
        "\n",
        "    print(\"ğŸ” Top reranked candidates:\")\n",
        "    for score, chunk in priority[:5]:\n",
        "        print(f\"âœ”ï¸ {chunk['name']} | Score: {score:.1f} | Dept: {chunk['department']}\")\n",
        "\n",
        "    return [x[1] for x in priority if x[0] > 0]\n",
        "\n",
        "# === Util: Structured Logic Fallback ===\n",
        "def structured_query_response(query):\n",
        "    query_lower = query.lower()\n",
        "    results = []\n",
        "\n",
        "    for prof in metadata:\n",
        "        dept_match = any(word in prof.get(\"department\", \"\").lower() for word in query_lower.split())\n",
        "        research_match = any(word in prof.get(\"research_interests\", \"\").lower() for word in query_lower.split()) or \\\n",
        "                         any(word in \" \".join(prof.get(\"research_keywords\", [])).lower() for word in query_lower.split())\n",
        "\n",
        "        if \"faculty\" in query_lower and (dept_match or research_match):\n",
        "            line = f\"{prof['name']} ({prof['designation']}, {prof['department']}): {prof['research_interests']}\"\n",
        "            results.append(line)\n",
        "\n",
        "    if results:\n",
        "        return \"Here are the relevant faculty:\\n\" + \"\\n\".join(\"â€¢ \" + r for r in results)\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "def truncate(text, max_words=400):\n",
        "    return \" \".join(text.split()[:max_words])\n",
        "\n",
        "def build_context(top_chunks):\n",
        "    return \"\\n\\n\".join(\n",
        "        [f\"{m['name']} ({m['designation']}, {m['department']}):\\n{m['content']}\" for m in top_chunks]\n",
        "    )\n",
        "\n",
        "# === Pipeline 1: Faculty Research Pipeline ===\n",
        "def faculty_pipeline(query, history=[]):\n",
        "    print(f\"\\nğŸ§  Incoming Faculty Query: {query}\")\n",
        "    q_embed = bert_model.encode([query], convert_to_numpy=True)\n",
        "    D, I = faiss_index.search(q_embed, k=10)\n",
        "    top_chunks = [metadata[i] for i in I[0]]\n",
        "\n",
        "    reranked_chunks = rerank_results(query, top_chunks)\n",
        "    top_chunks = reranked_chunks[:5] if reranked_chunks else top_chunks[:3]\n",
        "\n",
        "    if not reranked_chunks or len(reranked_chunks) < 2:\n",
        "        structured = structured_query_response(query)\n",
        "        if structured:\n",
        "            print(\"ğŸ“¦ Using structured query fallback.\")\n",
        "            return history + [(query, structured)]\n",
        "\n",
        "    context = truncate(build_context(top_chunks))\n",
        "    prompt = f\"\"\"You are a helpful academic assistant. Use the following context to answer.\n",
        "Only respond based on the faculty data. Do not make up facts.\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Question: {query}\n",
        "Answer:\"\"\"\n",
        "\n",
        "    inputs = phi_tokenizer(prompt, return_tensors=\"pt\").to(phi_model.device)\n",
        "    outputs = phi_model.generate(**inputs, max_new_tokens=300)\n",
        "    raw_output = phi_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    answer = raw_output.split(\"Answer:\")[-1].strip()\n",
        "\n",
        "    return history + [(query, answer)]\n",
        "\n",
        "# === Pipeline 2: Exam QA Pipeline ===\n",
        "def exam_qa_pipeline(query, history=[]):\n",
        "    print(f\"\\nğŸ§  Incoming Exam QA Query: {query}\")\n",
        "\n",
        "    # Let the user input \"Generate QA for subject: X, topic: Y\"\n",
        "    prompt = f\"\"\"You are a university teaching assistant helping students prepare for exams.\n",
        "\n",
        "{query}\n",
        "\n",
        "Generate exactly 10 multiple choice questions with answers.\n",
        "Each question should be clearly numbered 1 to 10 and followed by:\n",
        "A. ...\n",
        "B. ...\n",
        "C. ...\n",
        "D. ...\n",
        "Answer: <Correct option letter>\n",
        "\n",
        "Begin below:\n",
        "\"\"\"\n",
        "\n",
        "    inputs = phi_tokenizer(prompt, return_tensors=\"pt\").to(phi_model.device)\n",
        "    outputs = phi_model.generate(**inputs, max_new_tokens=700)\n",
        "    response = phi_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    response = response.split(\"Begin below:\")[-1].strip()\n",
        "\n",
        "    return history + [(query, response)]\n",
        "\n",
        "\n",
        "# === Router ===\n",
        "def gradio_chatbot(query, mode, history=[]):\n",
        "    print(f\"\\nğŸš¦ Mode Selected: {mode}\")\n",
        "    if mode == \"Exam QA Mode\":\n",
        "        return exam_qa_pipeline(query, history)\n",
        "    else:\n",
        "        return faculty_pipeline(query, history)\n",
        "\n",
        "# === Gradio UI ===\n",
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\"## ğŸ§  AI Teaching Assistant\\nChoose a mode and ask your question\")\n",
        "\n",
        "    mode_dropdown = gr.Dropdown(choices=[\"Faculty Research Mode\", \"Exam QA Mode\"], value=\"Faculty Research Mode\", label=\"Mode\")\n",
        "    chatbox = gr.Chatbot()\n",
        "    query_input = gr.Textbox(label=\"Ask your question...\")\n",
        "    clear = gr.Button(\"Clear Chat\")\n",
        "\n",
        "    query_input.submit(gradio_chatbot, inputs=[query_input, mode_dropdown, chatbox], outputs=chatbox)\n",
        "    clear.click(lambda: [], None, chatbox)\n",
        "\n",
        "demo.launch(debug=True, share=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3NIol66IA5PL"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LSDxxFcc93zJ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6KfLP2Sa-Nr_"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VSxGXsF4-NuV"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gbUThg3n-Nwt"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B4Iw3_E_-NzR"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "84jGKcUU-N1U"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0SiT9-rd-N3Y"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9tnNAOhY-N5m"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lq4zGR05-N8G"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ENq8j343-N9-"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H8y3uF3b-N_1"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cQUV59G0-OBs"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o_FVwJtD-ODe"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KOlsHfW5-OHE"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t3SrWFryw9ph"
      },
      "outputs": [],
      "source": [
        "# # === SECTION 3: Gradio Chat Interface ===\n",
        "# import gradio as gr\n",
        "\n",
        "# def rerank_results(query, chunks):\n",
        "#     priority = []\n",
        "\n",
        "#     for c in chunks:\n",
        "#         score = 0\n",
        "#         q = query.lower()\n",
        "\n",
        "#         if any(word in q for word in c.get(\"department\", \"\").lower().split()):\n",
        "#             score += 1\n",
        "#         if any(kw.lower() in q for kw in c.get(\"research_keywords\", [])):\n",
        "#             score += 1\n",
        "#         if c.get(\"research_interests\", \"\").lower() in q:\n",
        "#             score += 1\n",
        "#         if any(kw in q for kw in c.get(\"courses_taught\", [])):\n",
        "#             score += 0.5\n",
        "#         if any(word in q for word in c.get(\"ongoing_projects\", \"\").lower().split()):\n",
        "#             score += 0.5\n",
        "\n",
        "#         priority.append((score, c))\n",
        "\n",
        "#     # Sort by score descending\n",
        "#     priority.sort(reverse=True, key=lambda x: x[0])\n",
        "#     return [x[1] for x in priority if x[0] > 0]\n",
        "\n",
        "#     print(\"ğŸ” Top candidates post-rerank:\")\n",
        "#     for score, chunk in priority[:5]:\n",
        "#         print(f\"âœ”ï¸ {chunk['name']} | Score: {score} | Department: {chunk['department']}\")\n",
        "\n",
        "# def truncate(text, max_words=400):\n",
        "#     return \" \".join(text.split()[:max_words])\n",
        "\n",
        "# def build_context(top_chunks):\n",
        "#     return \"\\n\\n\".join(\n",
        "#         [f\"{m['name']} ({m['designation']}, {m['department']}):\\n{m['content']}\" for m in top_chunks]\n",
        "#     )\n",
        "\n",
        "# def structured_query_response(query):\n",
        "#     query_lower = query.lower()\n",
        "#     results = []\n",
        "\n",
        "#     for prof in metadata:\n",
        "#         dept_match = any(word in prof.get(\"department\", \"\").lower() for word in query_lower.split())\n",
        "#         research_match = any(word in prof.get(\"research_interests\", \"\").lower() for word in query_lower.split()) \\\n",
        "#             or any(word in \" \".join(prof.get(\"research_keywords\", [])).lower() for word in query_lower.split())\n",
        "\n",
        "#         if \"faculty\" in query_lower and (dept_match or research_match):\n",
        "#             line = f\"{prof['name']} ({prof['designation']}, {prof['department']}): {prof['research_interests']}\"\n",
        "#             results.append(line)\n",
        "\n",
        "#     if results:\n",
        "#         return \"Here are the relevant faculty:\\n\" + \"\\n\".join(\"â€¢ \" + r for r in results)\n",
        "#     else:\n",
        "#         return \"I'm unable to fetch any matching faculty from the data.\"\n",
        "\n",
        "\n",
        "\n",
        "# def gradio_chatbot(query, history=[]):\n",
        "#     q_embed = bert_model.encode([query], convert_to_numpy=True)\n",
        "#     D, I = faiss_index.search(q_embed, k=10)\n",
        "#     top_chunks = [metadata[i] for i in I[0]]\n",
        "\n",
        "#     reranked_chunks = rerank_results(query, top_chunks)\n",
        "#     top_chunks = reranked_chunks[:5] if reranked_chunks else top_chunks[:3]\n",
        "\n",
        "\n",
        "#     # Smart fallback to structured logic\n",
        "#     if \"faculty\" in query.lower() or \"professor\" in query.lower() or \"research\" in query.lower():\n",
        "#         structured_output = structured_query_response(query)\n",
        "#         if \"unable to fetch\" not in structured_output.lower():\n",
        "#             return history + [(query, structured_output)]\n",
        "\n",
        "#     context = truncate(build_context(top_chunks))\n",
        "#     prompt = f\"\"\"You are a helpful academic assistant. Use the following context to answer.\n",
        "# Only respond based on the faculty data. Do not make up facts.\n",
        "\n",
        "# Context:\n",
        "# {context}\n",
        "\n",
        "# Question: {query}\n",
        "# Answer:\"\"\"\n",
        "\n",
        "#     inputs = phi_tokenizer(prompt, return_tensors=\"pt\").to(phi_model.device)\n",
        "#     outputs = phi_model.generate(**inputs, max_new_tokens=300)\n",
        "#     raw_output = phi_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "#     answer = raw_output.split(\"Answer:\")[-1].strip()\n",
        "\n",
        "#     return history + [(query, answer)]\n",
        "\n",
        "# # === Gradio App ===\n",
        "# with gr.Blocks() as demo:\n",
        "#     gr.Markdown(\"## ğŸ“ Faculty Research Chatbot\\nAsk about departments, research, or projects\")\n",
        "\n",
        "#     chatbox = gr.Chatbot()\n",
        "#     query_input = gr.Textbox(label=\"Ask your question...\")\n",
        "#     clear = gr.Button(\"Clear Chat\")\n",
        "\n",
        "#     query_input.submit(gradio_chatbot, inputs=query_input, outputs=chatbox)\n",
        "#     clear.click(lambda: [], None, chatbox)\n",
        "\n",
        "# demo.launch(debug=True, share=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R_VFFI3fA6vn"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dHHTD-GTA6zD"
      },
      "outputs": [],
      "source": [
        "# === SECTION 3: Gradio Chat Interface ===\n",
        "import gradio as gr\n",
        "\n",
        "# === Utility: Reranking Function ===\n",
        "def rerank_results(query, chunks):\n",
        "    priority = []\n",
        "    query_lower = query.lower()\n",
        "\n",
        "    for c in chunks:\n",
        "        score = 0\n",
        "        dept = c.get(\"department\", \"\").lower()\n",
        "        r_keywords = [kw.lower() for kw in c.get(\"research_keywords\", [])]\n",
        "        r_interests = c.get(\"research_interests\", \"\").lower()\n",
        "        projects = c.get(\"ongoing_projects\", \"\").lower()\n",
        "        courses = [c_name.lower() for c_name in c.get(\"courses_taught\", []) if isinstance(c_name, str)]\n",
        "\n",
        "        if any(word in query_lower for word in dept.split()):\n",
        "            score += 1\n",
        "        if any(kw in query_lower for kw in r_keywords):\n",
        "            score += 1\n",
        "        if r_interests and r_interests in query_lower:\n",
        "            score += 1\n",
        "        if any(cname in query_lower for cname in courses):\n",
        "            score += 0.5\n",
        "        if any(word in query_lower for word in projects.split()):\n",
        "            score += 0.5\n",
        "\n",
        "        priority.append((score, c))\n",
        "\n",
        "    priority.sort(reverse=True, key=lambda x: x[0])\n",
        "\n",
        "    print(\"ğŸ” Top reranked candidates:\")\n",
        "    for score, chunk in priority[:5]:\n",
        "        print(f\"âœ”ï¸ {chunk['name']} | Score: {score:.1f} | Dept: {chunk['department']}\")\n",
        "\n",
        "    return [x[1] for x in priority if x[0] > 0]\n",
        "\n",
        "# === Utility: Text Builders ===\n",
        "def truncate(text, max_words=400):\n",
        "    return \" \".join(text.split()[:max_words])\n",
        "\n",
        "def build_context(top_chunks):\n",
        "    return \"\\n\\n\".join(\n",
        "        [f\"{m['name']} ({m['designation']}, {m['department']}):\\n{m['content']}\" for m in top_chunks]\n",
        "    )\n",
        "\n",
        "def structured_query_response(query):\n",
        "    query_lower = query.lower()\n",
        "    results = []\n",
        "\n",
        "    for prof in metadata:\n",
        "        dept_match = any(word in prof.get(\"department\", \"\").lower() for word in query_lower.split())\n",
        "        research_match = any(word in prof.get(\"research_interests\", \"\").lower() for word in query_lower.split()) or \\\n",
        "                         any(word in \" \".join(prof.get(\"research_keywords\", [])).lower() for word in query_lower.split())\n",
        "\n",
        "        if \"faculty\" in query_lower and (dept_match or research_match):\n",
        "            line = f\"{prof['name']} ({prof['designation']}, {prof['department']}): {prof['research_interests']}\"\n",
        "            results.append(line)\n",
        "\n",
        "    if results:\n",
        "        return \"Here are the relevant faculty:\\n\" + \"\\n\".join(\"â€¢ \" + r for r in results)\n",
        "    else:\n",
        "        return None  # Let LLM handle it\n",
        "\n",
        "# === Main Chat Function ===\n",
        "def gradio_chatbot(query, history=[]):\n",
        "    print(f\"\\nğŸ§  Incoming query: {query}\")\n",
        "    general_keywords = [\"create questions\", \"generate qa\", \"mcq\", \"exam\", \"interview questions\", \"question and answer\", \"prepare questions\"]\n",
        "\n",
        "    # === Handle general Q&A queries with Phi-2's own pretrained knowledge\n",
        "    if any(kw in query.lower() for kw in general_keywords):\n",
        "        print(\"ğŸ§  Using general knowledge mode (no FAISS context)\")\n",
        "        prompt = f\"\"\"You are a helpful AI tutor. Generate 10 exam-style questions and answers for:\n",
        "\n",
        "{query}\n",
        "\n",
        "Answer:\"\"\"\n",
        "        inputs = phi_tokenizer(prompt, return_tensors=\"pt\").to(phi_model.device)\n",
        "        outputs = phi_model.generate(**inputs, max_new_tokens=600)\n",
        "        raw_output = phi_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "        answer = raw_output.split(\"Answer:\")[-1].strip()\n",
        "        return history + [(query, answer)]\n",
        "\n",
        "    # === FAISS Retrieval\n",
        "    q_embed = bert_model.encode([query], convert_to_numpy=True)\n",
        "    D, I = faiss_index.search(q_embed, k=10)\n",
        "    top_chunks = [metadata[i] for i in I[0]]\n",
        "\n",
        "    # === Rerank\n",
        "    reranked_chunks = rerank_results(query, top_chunks)\n",
        "    top_chunks = reranked_chunks[:5] if reranked_chunks else top_chunks[:3]\n",
        "\n",
        "    # === Try rule-based answer if confidence is low\n",
        "    if not reranked_chunks or len(reranked_chunks) < 2:\n",
        "        structured = structured_query_response(query)\n",
        "        if structured:\n",
        "            print(\"ğŸ“¦ Using structured query fallback.\")\n",
        "            return history + [(query, structured)]\n",
        "\n",
        "    # === LLM-based Answer from faculty context\n",
        "    context = truncate(build_context(top_chunks))\n",
        "    prompt = f\"\"\"You are a helpful academic assistant. Use the following context to answer.\n",
        "Only respond based on the faculty data. Do not make up facts.\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Question: {query}\n",
        "Answer:\"\"\"\n",
        "\n",
        "    inputs = phi_tokenizer(prompt, return_tensors=\"pt\").to(phi_model.device)\n",
        "    outputs = phi_model.generate(**inputs, max_new_tokens=300)\n",
        "    raw_output = phi_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    answer = raw_output.split(\"Answer:\")[-1].strip()\n",
        "\n",
        "    return history + [(query, answer)]\n",
        "\n",
        "# === Gradio Interface ===\n",
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\"## ğŸ“ Faculty Research Chatbot\\nAsk about departments, research, or projects\")\n",
        "\n",
        "    chatbox = gr.Chatbot()\n",
        "    query_input = gr.Textbox(label=\"Ask your question...\")\n",
        "    clear = gr.Button(\"Clear Chat\")\n",
        "\n",
        "    query_input.submit(gradio_chatbot, inputs=query_input, outputs=chatbox)\n",
        "    clear.click(lambda: [], None, chatbox)\n",
        "\n",
        "demo.launch(debug=True, share=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8tgQthMfA7fu"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "6df12d9e0780469e8411a1e8c58550ff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_64a67343577f47939324a2fede432bd4",
              "IPY_MODEL_4c8f35d277fc484f8adc7e97426993d5",
              "IPY_MODEL_b76c9ed1c9854573ba56d948ae1cd8dc"
            ],
            "layout": "IPY_MODEL_fbc58c600c8e400d855a73833dc1602d"
          }
        },
        "64a67343577f47939324a2fede432bd4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_633b872f97e646ac8489f783ad18470a",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_d36ce85fe980450fae66df86eb55d746",
            "value": "Loadingâ€‡checkpointâ€‡shards:â€‡100%"
          }
        },
        "4c8f35d277fc484f8adc7e97426993d5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_abc8d5252cf1487ea8458ef25dc5412c",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_740ba14150d341a5a28876baa089ad3e",
            "value": 2
          }
        },
        "b76c9ed1c9854573ba56d948ae1cd8dc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dba1de8cf29c46e58f3b6a9c5073c855",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_8522b95c534e414a930bca9080252111",
            "value": "â€‡2/2â€‡[00:20&lt;00:00,â€‡â€‡9.01s/it]"
          }
        },
        "fbc58c600c8e400d855a73833dc1602d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "633b872f97e646ac8489f783ad18470a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d36ce85fe980450fae66df86eb55d746": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "abc8d5252cf1487ea8458ef25dc5412c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "740ba14150d341a5a28876baa089ad3e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "dba1de8cf29c46e58f3b6a9c5073c855": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8522b95c534e414a930bca9080252111": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}